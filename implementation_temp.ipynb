{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "import tqdm\n",
    "import time\n",
    "import datetime\n",
    "from scipy import sparse\n",
    "\n",
    "import polars as pl\n",
    "from polars import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFM1bDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        item_mapper,\n",
    "        user_mapper,\n",
    "        target=['country_encoded'],\n",
    "        fold_in=True,\n",
    "        split=\"train\",\n",
    "        conditioned_on=None,\n",
    "        upper=-1,\n",
    "    ):\n",
    "        super(LFM1bDataset, self).__init__()\n",
    "        assert os.path.exists(root), \"root: {} not found.\".format(root)\n",
    "        self.root = root\n",
    "\n",
    "        assert split in [\"test\", \"inference\", \"train\", \"valid\"]\n",
    "        self.split=split\n",
    "\n",
    "        out_data_dir = root\n",
    "        self.target=target\n",
    "        self.user_mapper = user_mapper\n",
    "        if self.split == \"train\":\n",
    "            self.train_data = pd.read_csv('/data/user_interactions_train.csv')\n",
    "        elif self.split == \"valid\":\n",
    "            self.vad_data_tr = pd.read_csv(root+'user_interactions_validation_tr.csv')\n",
    "            self.vad_data_te = pd.read_csv(root+'user_interactions_validation_te.csv')\n",
    "        elif self.split == \"test\":\n",
    "            self.test_data_tr = pd.read_csv(root+'user_interactions_test_tr.csv')\n",
    "            self.test_data_te = pd.read_csv(root+'user_interactions_test_te.csv')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    \n",
    "        if self.split == \"train\":\n",
    "            self.n_users = self.train_data.shape[0]\n",
    "        elif self.split == \"valid\":\n",
    "            self.n_users = self.vad_data_tr.shape[0]\n",
    "        elif self.split == \"test\":\n",
    "            self.n_users = self.test_data_tr.shape[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prof = np.zeros(1)\n",
    "        if self.split == \"train\":\n",
    "            data_tr, data_te = self.train_data.iloc[index].drop('user_id').to_numpy(dtype='float32'), np.zeros(1)\n",
    "            idx_user = self.train_data.at[index, 'user_id']\n",
    "        elif self.split == \"valid\":\n",
    "            # un comment line when vad_data_te is available\n",
    "            data_tr, data_te = self.vad_data_tr.iloc[index].drop('user_id').to_numpy(dtype='float32'), self.vad_data_te.iloc[index].drop('user_id').to_numpy(dtype='float32')\n",
    "            # data_tr, data_te = self.vad_data_tr.iloc[index].drop('user_id').to_numpy(dtype='float32'), np.zeros(1)\n",
    "            idx_user = self.vad_data_tr.at[index, 'user_id']\n",
    "        elif self.split == \"test\":\n",
    "            data_tr, data_te = self.test_data_tr.iloc[index].drop('user_id').to_numpy(dtype='float32'), self.test_data_te.iloc[index].drop('user_id').to_numpy(dtype='float32')\n",
    "            idx_user = self.test_data_tr.at[index, 'user_id']\n",
    "\n",
    "\n",
    "        \n",
    "        sensitive = self.user_mapper.loc[self.user_mapper.user_id == idx_user][\n",
    "            self.target\n",
    "        ].values[0]\n",
    "        \n",
    "        return data_tr, data_te, prof, idx_user, sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using polars instead of pandas\n",
    "class LFM1bDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        item_mapper,\n",
    "        user_mapper,\n",
    "        target=['country_encoded'],\n",
    "        fold_in=True,\n",
    "        split=\"train\",\n",
    "        conditioned_on=None,\n",
    "        upper=-1,\n",
    "    ):\n",
    "        super(LFM1bDataset, self).__init__()\n",
    "        assert os.path.exists(root), \"root: {} not found.\".format(root)\n",
    "        self.root = root\n",
    "\n",
    "        assert split in [\"test\", \"inference\", \"train\", \"valid\"]\n",
    "        self.split = split\n",
    "\n",
    "        out_data_dir = root\n",
    "        self.target = target\n",
    "        self.user_mapper = user_mapper\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.train_data = pl.read_csv('/data/user_interactions_train.csv', dtypes={'user_id': pl.Int32})\n",
    "        elif self.split == \"valid\":\n",
    "            self.vad_data_tr = pl.read_csv(root + 'user_interactions_validation_tr.csv', dtypes={'user_id': pl.Int32})\n",
    "            self.vad_data_te = pl.read_csv(root + 'user_interactions_validation_te.csv', dtypes={'user_id': pl.Int32})\n",
    "        elif self.split == \"test\":\n",
    "            self.test_data_tr = pl.read_csv(root + 'user_interactions_test_tr.csv', dtypes={'user_id': pl.Int32})\n",
    "            self.test_data_te = pl.read_csv(root + 'user_interactions_test_te.csv', dtypes={'user_id': pl.Int32})\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.n_users = self.train_data.shape[0]\n",
    "        elif self.split == \"valid\":\n",
    "            self.n_users = self.vad_data_tr.shape[0]\n",
    "        elif self.split == \"test\":\n",
    "            self.n_users = self.test_data_tr.shape[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prof = np.zeros(1)\n",
    "        if self.split == \"train\":\n",
    "            data_tr = self.train_data.row(index, named=True)\n",
    "            data_te = np.zeros(1)\n",
    "            idx_user = data_tr.pop(\"user_id\")\n",
    "            data_tr=np.fromiter(data_tr.values(),dtype='float32')\n",
    "        elif self.split == \"valid\":\n",
    "            data_tr = self.vad_data_tr.row(index, named=True)\n",
    "            data_te = self.vad_data_te.row(index, named=True)\n",
    "            data_te.pop(\"user_id\")\n",
    "            idx_user = data_tr.pop(\"user_id\")\n",
    "            data_tr=np.fromiter(data_tr.values(),dtype='float32')\n",
    "            data_te=np.fromiter(data_te.values(),dtype='float32')\n",
    "        elif self.split == \"test\":\n",
    "            data_tr = self.test_data_tr.row(index, named=True)\n",
    "            data_te = self.test_data_te.row(index, named=True)\n",
    "            data_te.pop(\"user_id\")\n",
    "            idx_user = data_tr.pop(\"user_id\")\n",
    "            data_tr=np.fromiter(data_tr.values(),dtype='float32')\n",
    "            data_te=np.fromiter(data_te.values(),dtype='float32')\n",
    "\n",
    "        sensitive = self.user_mapper.loc[self.user_mapper.user_id == idx_user][\n",
    "            self.target\n",
    "        ].to_numpy()[0]\n",
    "\n",
    "        return data_tr, data_te, prof, idx_user, sensitive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Buzairahmd.rd.tuni.fi/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLFM1bDataset\u001b[39;00m(data\u001b[39m.\u001b[39mDataset):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buzairahmd.rd.tuni.fi/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buzairahmd.rd.tuni.fi/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Buzairahmd.rd.tuni.fi/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         root,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buzairahmd.rd.tuni.fi/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         upper\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buzairahmd.rd.tuni.fi/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     ):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Buzairahmd.rd.tuni.fi/home/mduzah/Thesis_work/thesis/implementation_temp.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39msuper\u001b[39m(LFM1bDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Reading h5 format\n",
    "class LFM1bDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        item_mapper,\n",
    "        user_mapper,\n",
    "        target=['country_encoded'],\n",
    "        fold_in=True,\n",
    "        split=\"train\",\n",
    "        conditioned_on=None,\n",
    "        upper=-1,\n",
    "    ):\n",
    "        super(LFM1bDataset, self).__init__()\n",
    "        assert os.path.exists(root), \"root: {} not found.\".format(root)\n",
    "        self.root = root\n",
    "\n",
    "        assert split in [\"test\", \"inference\", \"train\", \"valid\"]\n",
    "        self.split=split\n",
    "\n",
    "        out_data_dir = root\n",
    "        self.target=target\n",
    "        self.user_mapper = user_mapper\n",
    "        if self.split == \"train\":\n",
    "            self.train_data = pd.read_hdf(root+'user_interactions_train.h5',key=\"user_train\")\n",
    "        elif self.split == \"valid\":\n",
    "            self.vad_data_tr = pd.read_hdf(root+'user_interactions_validation_tr.h5',key=\"user_valid_tr\")\n",
    "            self.vad_data_te = pd.read_hdf(root+'user_interactions_validation_te.h5',key=\"user_valid_te\")\n",
    "        elif self.split == \"test\":\n",
    "            self.test_data_tr = pd.read_hdf(root+'user_interactions_test_tr.h5',key=\"user_test_tr\")\n",
    "            self.test_data_te = pd.read_hdf(root+'user_interactions_test_te.h5',key=\"user_test_te\")\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    \n",
    "        if self.split == \"train\":\n",
    "            self.n_users = self.train_data.shape[0]\n",
    "        elif self.split == \"valid\":\n",
    "            self.n_users = self.vad_data_tr.shape[0]\n",
    "        elif self.split == \"test\":\n",
    "            self.n_users = self.test_data_tr.shape[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prof = np.zeros(1)\n",
    "        if self.split == \"train\":\n",
    "            data_tr, data_te = self.train_data.iloc[index].to_numpy(dtype='float32'), np.zeros(1)\n",
    "            idx_user = self.train_data.index[index]\n",
    "        elif self.split == \"valid\":\n",
    "            # un comment line when vad_data_te is available\n",
    "            data_tr, data_te = self.vad_data_tr.iloc[index].to_numpy(dtype='float32'), self.vad_data_te.iloc[index].to_numpy(dtype='float32')\n",
    "            # data_tr, data_te = self.vad_data_tr.iloc[index].drop('user_id').to_numpy(dtype='float32'), np.zeros(1)\n",
    "            idx_user = self.vad_data_tr.index[index]\n",
    "        elif self.split == \"test\":\n",
    "            data_tr, data_te = self.test_data_tr.iloc[index].to_numpy(dtype='float32'), self.test_data_te.iloc[index].to_numpy(dtype='float32')\n",
    "            idx_user = self.test_data_tr.index[index]\n",
    "\n",
    "\n",
    "        \n",
    "        sensitive = self.user_mapper.loc[self.user_mapper.user_id == idx_user][\n",
    "            self.target\n",
    "        ].values[0]\n",
    "        \n",
    "        return data_tr, data_te, prof, idx_user, sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mapper = pd.read_csv('./Data/items.csv')\n",
    "user_mapper = pd.read_csv('./Data/users.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, options, dropout_p=0.5, q_dims=[20108, 600, 200]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.options = options\n",
    "        self.q_dims = q_dims\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p, inplace=False)\n",
    "        self.linear_1 = nn.Linear(self.q_dims[0], self.q_dims[1], bias=True)\n",
    "        self.linear_2 = nn.Linear(self.q_dims[1], self.q_dims[2] * 2, bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        for module_name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear_1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear_2(x)\n",
    "        mu_q, logvar_q = torch.chunk(x, chunks=2, dim=1)\n",
    "        return mu_q, logvar_q\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, options, p_dims=[200, 600, 20108]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.options = options\n",
    "        self.p_dims = p_dims\n",
    "\n",
    "        self.linear_1 = nn.Linear(self.p_dims[0], self.p_dims[1], bias=True)\n",
    "        self.linear_2 = nn.Linear(self.p_dims[1], self.p_dims[2], bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        for module_name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiVAE(nn.Module):\n",
    "    def __init__(self, cuda2=True, weight_decay=0.0, dropout_p=0.5, q_dims=[20108, 600, 200], p_dims=[200, 600, 20108], n_conditioned=0, n_sensitive_attributes=1,n_classes=2):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.cuda2 = cuda2\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_conditioned = n_conditioned\n",
    "        self.q_dims = q_dims\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims[0] += self.n_conditioned\n",
    "        self.p_dims[0] += self.n_conditioned\n",
    "\n",
    "        self.encoder = Encoder(None, dropout_p=dropout_p, q_dims=self.q_dims)\n",
    "        self.decoder = Decoder(None, p_dims=self.p_dims)\n",
    "\n",
    "        self.classify = nn.Linear(200, n_classes, bias=True)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x = f.normalize(x, p=2, dim=1)\n",
    "        if self.n_conditioned > 0:\n",
    "            x = torch.cat((x, c), dim=1)\n",
    "        mu_q, logvar_q = self.encoder.forward(x)\n",
    "        std_q = torch.exp(0.5 * logvar_q)\n",
    "        KL = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q ** 2 - 1), dim=1))\n",
    "\n",
    "        if True:\n",
    "            if self.training:\n",
    "                epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "                sampled_z = mu_q + epsilon * std_q\n",
    "            else:\n",
    "                epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "                sampled_z = mu_q\n",
    "        else:\n",
    "            epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "            sampled_z = mu_q + epsilon * std_q\n",
    "\n",
    "        if self.n_conditioned > 0:\n",
    "            sampled_z = torch.cat((sampled_z, c), dim=1)\n",
    "        logits = self.decoder.forward(sampled_z)\n",
    "\n",
    "        return logits, KL, mu_q, std_q, epsilon, sampled_z\n",
    "\n",
    "    def get_l2_reg(self):\n",
    "        l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "        if self.weight_decay > 0:\n",
    "            for k, m in self.state_dict().items():\n",
    "                if k.endswith('.weight'):\n",
    "                    l2_reg = l2_reg + torch.norm(m, p=2) ** 2\n",
    "        if self.cuda2:\n",
    "            l2_reg = l2_reg.cuda()\n",
    "        return self.weight_decay * l2_reg[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = LFM1bDataset\n",
    "dt = DS(\"./Data/\", item_mapper, user_mapper, target=[\"country_encoded\"], split=\"train\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dt, batch_size=500, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DS(\n",
    "    \"./Data/\",\n",
    "    item_mapper,\n",
    "    user_mapper,\n",
    "    target=[\"country_encoded\"],\n",
    "    split=\"valid\",\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dv, batch_size=200, shuffle=False, num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cmd,\n",
    "        cuda,\n",
    "        model,\n",
    "        optim=None,\n",
    "        train_loader=None,\n",
    "        valid_loader=None,\n",
    "        test_loader=None,\n",
    "        log_file=None,\n",
    "        interval_validate=1,\n",
    "        lr_scheduler=None,\n",
    "        dataset_name=None,\n",
    "        gamma=0.0,\n",
    "        tau=0.0,\n",
    "        start_step=0,\n",
    "        total_steps=1e5,\n",
    "        start_epoch=0,\n",
    "        bias=False,\n",
    "        target=None,\n",
    "        total_anneal_steps=200000,\n",
    "        beta=0.1,\n",
    "        do_normalize=True,\n",
    "        item_mapper=None,\n",
    "        user_mapper=None,\n",
    "        checkpoint_dir=None,\n",
    "        result_dir=None,\n",
    "        print_freq=1,\n",
    "        result_save_freq=1,\n",
    "        checkpoint_freq=1,\n",
    "        base_dir=None,\n",
    "    ):\n",
    "        self.cmd = cmd\n",
    "        self.cuda = cuda\n",
    "        self.model = model\n",
    "        self.item_mapper = item_mapper\n",
    "        self.user_mapper = user_mapper\n",
    "        self.dataset_name = dataset_name\n",
    "        self.bias = bias\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        self.optim = optim\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.timestamp_start = datetime.datetime.now()\n",
    "\n",
    "        if self.cmd == \"train\":\n",
    "            self.interval_validate = interval_validate\n",
    "\n",
    "        self.start_step = start_step\n",
    "        self.step = start_step\n",
    "        self.total_steps = total_steps\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "        self.do_normalize = do_normalize\n",
    "        self.print_freq = print_freq\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.total_anneal_steps = total_anneal_steps\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.ndcg, self.recall, self.ash, self.amt, self.alt, self.ent, self.demo = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        self.loss, self.kl, self.posb, self.popb = [], [], [], []\n",
    "        self.neg, self.kl, self.ubias = [], [], []\n",
    "\n",
    "        self.target = target\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def validate(self, cmd=\"valid\", k=100):\n",
    "        assert cmd in [\"valid\", \"test\"]\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        self.model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        n10_list, n100_list, r10_list, r100_list = [], [], [], []\n",
    "        embs_list = []\n",
    "        att_round, rel_round, cnt_round, pcount_round, udx_list = [], [], [], [], []\n",
    "        result = []\n",
    "        eval_loss = 0.0\n",
    "        eval_neg = 0.0\n",
    "        eval_kl = 0.0\n",
    "        eval_ubias = 0.0\n",
    "\n",
    "        loader_ = self.valid_loader if cmd == \"valid\" else self.test_loader\n",
    "\n",
    "        step_counter = 0\n",
    "        for batch_idx, (data_tr, data_te, prof, uindex, sens) in tqdm.tqdm(\n",
    "            enumerate(loader_),\n",
    "            total=len(loader_),\n",
    "            desc=\"{} check epoch={}, len={}\".format(\n",
    "                \"Valid\" if cmd == \"valid\" else \"Test\", self.epoch, len(loader_)\n",
    "            ),\n",
    "            ncols=80,\n",
    "            leave=False,\n",
    "        ):\n",
    "            step_counter = step_counter + 1\n",
    "\n",
    "            if self.cuda:\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "                sens = sens.cuda()\n",
    "            data_tr = Variable(data_tr)\n",
    "            prof = Variable(prof)\n",
    "            data_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, KL, mu_q, std_q, epsilon, sampled_z = self.model.forward(\n",
    "                    data_tr, prof\n",
    "                )\n",
    "\n",
    "                log_softmax_var = f.log_softmax(logits, dim=1)\n",
    "                neg_ll = -torch.mean(torch.sum(log_softmax_var * data_tr, dim=1))\n",
    "                eval_neg += neg_ll.item()\n",
    "                eval_kl += KL.item()\n",
    "\n",
    "                user_bias = utils.calc_user_bias(\n",
    "                    torch.sum(log_softmax_var * data_tr, dim=1), sens\n",
    "                )\n",
    "                eval_ubias += user_bias.item()\n",
    "\n",
    "                ## innner classifier loss\n",
    "                y_hat = self.model.classify(sampled_z)\n",
    "                if self.cuda:\n",
    "                    class_loss = self.criterion(\n",
    "                        y_hat,\n",
    "                        torch.flatten(Variable(sens.type(torch.LongTensor))).cuda(),\n",
    "                    )\n",
    "                else:\n",
    "                    class_loss = self.criterion(\n",
    "                        y_hat, torch.flatten(Variable(sens.type(torch.LongTensor)))\n",
    "                    )\n",
    "\n",
    "                eval_loss += class_loss.item()\n",
    "\n",
    "                pred_val = logits.cpu().detach().numpy()\n",
    "                pred_val[data_tr.cpu().detach().numpy().nonzero()] = -np.inf\n",
    "\n",
    "                data_te_csr = sparse.csr_matrix(data_te.numpy())\n",
    "                n10_list.append(\n",
    "                    utils.NDCG_binary_at_k_batch(pred_val, data_te_csr, k=10)\n",
    "                )\n",
    "                n100_list.append(\n",
    "                    utils.NDCG_binary_at_k_batch(pred_val, data_te_csr, k=100)\n",
    "                )\n",
    "                r10_list.append(utils.Recall_at_k_batch(pred_val, data_te_csr, k=10))\n",
    "                r100_list.append(utils.Recall_at_k_batch(pred_val, data_te_csr, k=100))\n",
    "\n",
    "                if cmd == \"test\":\n",
    "                    for user in np.arange(data_te.numpy().shape[0]):\n",
    "                        dict_out = {}\n",
    "                        preds = pred_val[user, :]\n",
    "\n",
    "                        dict_out[\"num_missing_terms\"] = len(\n",
    "                            np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                        )\n",
    "                        dict_out[\"missing_terms\"] = \" \".join(\n",
    "                            [\n",
    "                                str(x)\n",
    "                                for x in list(\n",
    "                                    np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                        dict_out[\"num_terms\"] = len(\n",
    "                            np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                        ) + len(\n",
    "                            np.array(data_tr.cpu().detach().numpy()[user, :]).nonzero()[\n",
    "                                0\n",
    "                            ]\n",
    "                        )\n",
    "                        dict_out[\"recommended_terms\"] = \" \".join(\n",
    "                            [str(x) for x in list(np.argsort(-preds)[:k])]\n",
    "                        )\n",
    "                        dict_out[\"user_id\"] = int(uindex[user].cpu().detach().numpy())\n",
    "                        dict_out[\"scores\"] = \" \".join(\n",
    "                            [\n",
    "                                str(x)\n",
    "                                for x in list(np.sort(self.softmax(preds))[::-1][:k])\n",
    "                            ]\n",
    "                        )\n",
    "                        result.append(dict_out)\n",
    "\n",
    "        avg_loss = eval_loss / len(loader_)\n",
    "        avg_neg = eval_neg / len(loader_)\n",
    "        avg_kl = eval_kl / len(loader_)\n",
    "        avg_ubias = eval_ubias / len(loader_)\n",
    "\n",
    "        metrics = []\n",
    "        if cmd == \"valid\":\n",
    "            n10_list = np.concatenate(n10_list, axis=0)\n",
    "            n100_list = np.concatenate(n100_list, axis=0)\n",
    "            r10_list = np.concatenate(r10_list, axis=0)\n",
    "            r100_list = np.concatenate(r100_list, axis=0)\n",
    "\n",
    "            self.ndcg.append(np.mean(n100_list))\n",
    "            self.recall.append(np.mean(r100_list))\n",
    "            self.loss.append(avg_loss)\n",
    "            self.neg.append(avg_neg)\n",
    "            self.kl.append(avg_kl)\n",
    "            self.ubias.append(avg_ubias)\n",
    "\n",
    "            np.save(\n",
    "                \"/data/results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_ndcg_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.ndcg,\n",
    "            )\n",
    "            np.save(\n",
    "                \"/data/results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_recall_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.recall,\n",
    "            )\n",
    "            np.save(\n",
    "                \"/data/results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_loss_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.loss,\n",
    "            )\n",
    "            np.save(\n",
    "                \"/data/results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_neg_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.neg,\n",
    "            )\n",
    "            np.save(\n",
    "                \"/data/results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_kl_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.kl,\n",
    "            )\n",
    "            np.save(\n",
    "                \"/data/results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_ubias_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.ubias,\n",
    "            )\n",
    "\n",
    "            # SAVE MODEL\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": self.epoch,\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optim.state_dict(),\n",
    "                },\n",
    "                self.checkpoint_dir\n",
    "                + self.dataset_name\n",
    "                + \"_vae_{}_{}_{}_{}.pth\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "            )\n",
    "            # with open(self.checkpoint_dir+self.dataset_name+'_vae_'+str(self.bias)+'_'+str(self.alpha)+'.pt', 'wb') as model_file: torch.save(self.model, model_file)\n",
    "            # torch.save({'state_dict': self.model.state_dict()}, self.checkpoint_dir+'vae')\n",
    "\n",
    "            metrics.append(\n",
    "                \"NDCG@10,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(n10_list), np.std(n10_list) / np.sqrt(len(n10_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"NDCG@100,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"Recall@10,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(r10_list), np.std(r10_list) / np.sqrt(len(r10_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"Recall@100,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(r100_list), np.std(r100_list) / np.sqrt(len(r100_list))\n",
    "                )\n",
    "            )\n",
    "            print(\"\\n\" + \",\\n\".join(metrics))\n",
    "\n",
    "        else:\n",
    "            final_results = pd.DataFrame(result)\n",
    "            final_results = final_results.merge(\n",
    "                self.user_mapper[[\"user_id\", \"sex\", \"country\", \"age\"]],\n",
    "                on=\"user_id\",\n",
    "                how=\"inner\",\n",
    "            )\n",
    "            final_results.to_csv(\n",
    "                \"/data/results/{}_final_results_{}_{}_{}_{}.csv\".format(\n",
    "                    self.dataset_name, self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        cmd = \"train\"\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for batch_idx, (data_tr, data_te, prof, uidx, sens) in tqdm.tqdm(\n",
    "            enumerate(self.train_loader),\n",
    "            total=len(self.train_loader),\n",
    "            desc=\"Train check epoch={}, len={}\".format(\n",
    "                self.epoch, len(self.train_loader)\n",
    "            ),\n",
    "            ncols=80,\n",
    "            leave=False,\n",
    "        ):\n",
    "            self.step += 1\n",
    "\n",
    "            if self.cuda:\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "                sens = sens.cuda()  # added by me\n",
    "            data_tr = Variable(data_tr)\n",
    "            prof = Variable(prof)\n",
    "            data_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            logits, KL, mu_q, std_q, epsilon, sampled_z = self.model.forward(\n",
    "                data_tr, prof\n",
    "            )\n",
    "            log_softmax_var = f.log_softmax(logits, dim=1)\n",
    "            neg_ll = -torch.mean(torch.sum(log_softmax_var * data_tr, dim=1))\n",
    "\n",
    "            l2_reg = self.model.get_l2_reg()\n",
    "\n",
    "            if self.total_anneal_steps > 0:\n",
    "                self.anneal = min(self.beta, 1.0 * self.step / self.total_anneal_steps)\n",
    "            else:\n",
    "                self.anneal = self.beta\n",
    "\n",
    "            ## CLASSIFICATION ACCURACY\n",
    "            y_hat = self.model.classify(sampled_z)\n",
    "            if self.cuda:\n",
    "                class_loss = self.criterion(\n",
    "                    y_hat, torch.flatten(Variable(sens.type(torch.LongTensor))).cuda()\n",
    "                )\n",
    "            else:\n",
    "                class_loss = self.criterion(\n",
    "                    y_hat, torch.flatten(Variable(sens.type(torch.LongTensor)))\n",
    "                )\n",
    "\n",
    "            # USER BIAS\n",
    "            user_bias = utils.calc_user_bias(\n",
    "                torch.sum(log_softmax_var * data_tr, dim=1), sens\n",
    "            )\n",
    "\n",
    "            loss = (\n",
    "                neg_ll\n",
    "                + self.anneal * KL\n",
    "                + l2_reg\n",
    "                - self.gamma * class_loss\n",
    "                + self.tau * user_bias\n",
    "            )\n",
    "            # backprop\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def train(self):\n",
    "        max_epoch = 100\n",
    "        for epoch in tqdm.trange(0, max_epoch, desc=\"Train\", ncols=80):\n",
    "            self.epoch = epoch\n",
    "            self.train_epoch()\n",
    "            self.lr_scheduler.step()\n",
    "            self.validate(cmd=\"valid\")\n",
    "            # self.validate(cmd='test')\n",
    "\n",
    "    def test(self):\n",
    "        self.validate(cmd=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"torch.backends.cudnn.version: {}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "if not os.path.isdir(\"/data/checkpoint\"):\n",
    "    os.mkdir(\"/data/checkpoint\")\n",
    "if not os.path.isdir(\"/data/results\"):\n",
    "    os.mkdir(\"/data/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=dict(\n",
    "        max_iteration=1000000,\n",
    "        lr=1e-4,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0,\n",
    "        gamma=0.1,  # \"lr_policy: step\"\n",
    "        step_size=200000,  # \"lr_policy: step\" e-6\n",
    "        interval_validate=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiVAE(\n",
    "    dropout_p=0.5,\n",
    "    weight_decay=0.0,\n",
    "    cuda2=cuda,\n",
    "    q_dims=[item_mapper.shape[0], 2000, 200],\n",
    "    p_dims=[200, 2000, item_mapper.shape[0]],\n",
    "    n_conditioned=0,\n",
    "    n_sensitive_attributes=1, # only country for now\n",
    "    n_classes=user_mapper['country'].nunique()\n",
    ")\n",
    "# 3. optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=False)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=True)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ],\n",
    "    lr=cfg[\"lr\"],\n",
    ")\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_policy: step\n",
    "last_epoch = -1\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[50, 75], gamma=cfg[\"gamma\"], last_epoch=last_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    cmd=\"train\",\n",
    "    cuda=cuda,\n",
    "    model=model,\n",
    "    optim=optim,\n",
    "    gamma=0.5,\n",
    "    tau=0.5,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    # test_loader=test_loader,\n",
    "    start_step=0,\n",
    "    total_steps=int(3e5),\n",
    "    interval_validate=None,\n",
    "    checkpoint_dir=\"/data/checkpoint/\",\n",
    "    print_freq=1,\n",
    "    checkpoint_freq=1,\n",
    "    total_anneal_steps=2000,\n",
    "    beta=0.5,\n",
    "    item_mapper=item_mapper,\n",
    "    user_mapper=user_mapper,\n",
    "    dataset_name=\"lfm2b\",\n",
    "    # alpha=0.5,\n",
    "    base_dir=\"./Data/\",\n",
    "    target=[\"country_encoded\"],)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = LFM1bDataset\n",
    "\n",
    "df_test = DS(\n",
    "    \"./Data/\",\n",
    "    item_mapper,\n",
    "    user_mapper,\n",
    "    target=[\"country_encoded\"],\n",
    "    split=\"test\",\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    df_test,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiVAE(\n",
    "    dropout_p=0.5,\n",
    "    weight_decay=0.0,\n",
    "    cuda2=cuda,\n",
    "    q_dims=[item_mapper.shape[0], 2000, 200],\n",
    "    p_dims=[200, 2000, item_mapper.shape[0]],\n",
    "    n_conditioned=0,\n",
    "    n_sensitive_attributes=1,  # only country for now\n",
    "    n_classes=user_mapper['country'].nunique()\n",
    ")\n",
    "# 3. optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=False)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=True)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ],\n",
    "    lr=cfg[\"lr\"],\n",
    ")\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    \"checkpoint/{}_vae_{}_{}_{}_{}.pth\".format(\n",
    "        \"lfm2b\", [\"country_encoded\"], 0.5, 0.5, 0.5\n",
    "    )\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optim.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "epoch = checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = -1\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[50, 75], gamma=cfg[\"gamma\"], last_epoch=last_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    cmd=\"test\",\n",
    "    cuda=cuda,\n",
    "    model=model,\n",
    "    optim=optim,\n",
    "    gamma=0.5,\n",
    "    tau=0.5,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    train_loader=None,\n",
    "    valid_loader=None,\n",
    "    test_loader=test_loader,\n",
    "    start_step=0,\n",
    "    total_steps=int(3e5),\n",
    "    interval_validate=None,\n",
    "    checkpoint_dir=\"./checkpoint/\",\n",
    "    print_freq=1,\n",
    "    checkpoint_freq=1,\n",
    "    total_anneal_steps=2000,\n",
    "    beta=0.5,\n",
    "    item_mapper=item_mapper,\n",
    "    user_mapper=user_mapper,\n",
    "    dataset_name=\"lfm2b\",\n",
    "    # alpha=0.5,\n",
    "    base_dir=\"./Data/\",\n",
    "    target=[\"country_encoded\"],\n",
    ")\n",
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
