{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "import tqdm\n",
    "import time\n",
    "import datetime\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFM1bDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        item_mapper,\n",
    "        user_mapper,\n",
    "        target=['country_encoded'],\n",
    "        fold_in=True,\n",
    "        split=\"train\",\n",
    "        conditioned_on=None,\n",
    "        upper=-1,\n",
    "    ):\n",
    "        super(LFM1bDataset, self).__init__()\n",
    "        assert os.path.exists(root), \"root: {} not found.\".format(root)\n",
    "        self.root = root\n",
    "\n",
    "        assert split in [\"test\", \"inference\", \"train\", \"valid\"]\n",
    "        self.split=split\n",
    "\n",
    "        out_data_dir = root\n",
    "        self.target=target\n",
    "        self.user_mapper = user_mapper\n",
    "        self.train_data = pd.read_csv(root+'user_interactions_train.csv')\n",
    "        self.vad_data_tr = pd.read_csv(root+'user_interactions_validation.csv')\n",
    "\n",
    "\n",
    "        unique_sid = item_mapper['song_id'].unique()\n",
    "\n",
    "        n_items = len(unique_sid)\n",
    "\n",
    "\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.n_users = self.train_data.shape[0]\n",
    "        elif self.split == \"valid\":\n",
    "            self.n_users = self.vad_data_tr.shape[0]\n",
    "        elif self.split == \"test\":\n",
    "            self.n_users = self.test_data_tr.shape[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prof = np.zeros(1)\n",
    "        if self.split == \"train\":\n",
    "            data_tr, data_te = self.train_data.iloc[index].drop('user_id').to_numpy(dtype='float32'), np.zeros(1)\n",
    "            idx_user = self.train_data.at[index, 'user_id']\n",
    "        elif self.split == \"valid\":\n",
    "            # un comment line when vad_data_te is available\n",
    "            # data_tr, data_te = self.vad_data_tr.iloc[index].drop('user_id').to_numpy(dtype='float32'), self.vad_data_te[index]\n",
    "            data_tr, data_te = self.vad_data_tr.iloc[index].drop('user_id').to_numpy(dtype='float32'), np.zeros(1)\n",
    "            idx_user = self.vad_data_tr.at[index, 'user_id']\n",
    "        elif self.split == \"test\":\n",
    "            data_tr, data_te = self.test_data_tr[index], self.test_data_te[index]\n",
    "            idx_user = self.te_idx[index]\n",
    "\n",
    "\n",
    "        \n",
    "        sensitive = self.user_mapper.loc[self.user_mapper.user_id == idx_user][\n",
    "            self.target\n",
    "        ].values[0]\n",
    "        \n",
    "        return data_tr, data_te, prof, idx_user, sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mapper = pd.read_csv('./Data/items.csv')\n",
    "user_mapper = pd.read_csv('./Data/users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SENSITIVE_ATTR_VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=200, n_sensitive_attributes=1, hidden_dim=100, latent_dim=50):\n",
    "        super(SENSITIVE_ATTR_VAE, self).__init__()\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, 2)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 2)\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, n_sensitive_attributes),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "     \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(self.device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, options, dropout_p=0.5, q_dims=[20108, 600, 200]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.options = options\n",
    "        self.q_dims = q_dims\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p, inplace=False)\n",
    "        self.linear_1 = nn.Linear(self.q_dims[0], self.q_dims[1], bias=True)\n",
    "        self.linear_2 = nn.Linear(self.q_dims[1], self.q_dims[2] * 2, bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        for module_name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear_1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear_2(x)\n",
    "        mu_q, logvar_q = torch.chunk(x, chunks=2, dim=1)\n",
    "        return mu_q, logvar_q\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, options, p_dims=[200, 600, 20108]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.options = options\n",
    "        self.p_dims = p_dims\n",
    "\n",
    "        self.linear_1 = nn.Linear(self.p_dims[0], self.p_dims[1], bias=True)\n",
    "        self.linear_2 = nn.Linear(self.p_dims[1], self.p_dims[2], bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        for module_name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiVAE(nn.Module):\n",
    "    def __init__(self, cuda2=True, weight_decay=0.0, dropout_p=0.5, q_dims=[20108, 600, 200], p_dims=[200, 600, 20108], n_conditioned=0, n_sensitive_attributes=1):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.cuda2 = cuda2\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_conditioned = n_conditioned\n",
    "        self.q_dims = q_dims\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims[0] += self.n_conditioned\n",
    "        self.p_dims[0] += self.n_conditioned\n",
    "\n",
    "        self.encoder = Encoder(None, dropout_p=dropout_p, q_dims=self.q_dims)\n",
    "        self.decoder = Decoder(None, p_dims=self.p_dims)\n",
    "\n",
    "        self.sensitive_atr_vae = SENSITIVE_ATTR_VAE(n_sensitive_attributes=n_sensitive_attributes)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x = f.normalize(x, p=2, dim=1)\n",
    "        if self.n_conditioned > 0:\n",
    "            x = torch.cat((x, c), dim=1)\n",
    "        mu_q, logvar_q = self.encoder.forward(x)\n",
    "        std_q = torch.exp(0.5 * logvar_q)\n",
    "        KL = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q ** 2 - 1), dim=1))\n",
    "\n",
    "        if True:\n",
    "            if self.training:\n",
    "                epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "                sampled_z = mu_q + epsilon * std_q\n",
    "            else:\n",
    "                epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "                sampled_z = mu_q\n",
    "        else:\n",
    "            epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "            sampled_z = mu_q + epsilon * std_q\n",
    "\n",
    "        if self.n_conditioned > 0:\n",
    "            sampled_z = torch.cat((sampled_z, c), dim=1)\n",
    "        logits = self.decoder.forward(sampled_z)\n",
    "\n",
    "        return logits, KL, mu_q, std_q, epsilon, sampled_z\n",
    "\n",
    "    def get_l2_reg(self):\n",
    "        l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "        if self.weight_decay > 0:\n",
    "            for k, m in self.state_dict().items():\n",
    "                if k.endswith('.weight'):\n",
    "                    l2_reg = l2_reg + torch.norm(m, p=2) ** 2\n",
    "        if self.cuda2:\n",
    "            l2_reg = l2_reg.cuda()\n",
    "        return self.weight_decay * l2_reg[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = LFM1bDataset\n",
    "dt = DS(\n",
    "    './Data/',\n",
    "    item_mapper,\n",
    "    user_mapper,\n",
    "    target=['country_encoded'],\n",
    "    split=\"train\"\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dt, batch_size=500, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DS(\n",
    "'./Data/',\n",
    "item_mapper,\n",
    "user_mapper,\n",
    "target=['country_encoded'],\n",
    "split=\"valid\",\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "dt, batch_size=100, shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cmd,\n",
    "        cuda,\n",
    "        model,\n",
    "        optim=None,\n",
    "        train_loader=None,\n",
    "        valid_loader=None,\n",
    "        test_loader=None,\n",
    "        log_file=None,\n",
    "        interval_validate=1,\n",
    "        lr_scheduler=None,\n",
    "        dataset_name=None,\n",
    "        gamma=0.0,\n",
    "        tau=0.0,\n",
    "        start_step=0,\n",
    "        total_steps=1e5,\n",
    "        start_epoch=0,\n",
    "        bias=False,\n",
    "        target=None,\n",
    "        total_anneal_steps=200000,\n",
    "        beta=0.1,\n",
    "        do_normalize=True,\n",
    "        item_mapper=None,\n",
    "        user_mapper=None,\n",
    "        checkpoint_dir=None,\n",
    "        result_dir=None,\n",
    "        print_freq=1,\n",
    "        result_save_freq=1,\n",
    "        checkpoint_freq=1,\n",
    "        base_dir=None,\n",
    "    ):\n",
    "        self.cmd = cmd\n",
    "        self.cuda = cuda\n",
    "        self.model = model\n",
    "        self.item_mapper = item_mapper\n",
    "        self.user_mapper = user_mapper\n",
    "        self.dataset_name = dataset_name\n",
    "        self.bias = bias\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        self.optim = optim\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.timestamp_start = datetime.datetime.now()\n",
    "\n",
    "        if self.cmd == \"train\":\n",
    "            self.interval_validate = interval_validate\n",
    "\n",
    "        self.start_step = start_step\n",
    "        self.step = start_step\n",
    "        self.total_steps = total_steps\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "        self.do_normalize = do_normalize\n",
    "        self.print_freq = print_freq\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.total_anneal_steps = total_anneal_steps\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.ndcg, self.recall, self.ash, self.amt, self.alt, self.ent, self.demo = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        self.loss, self.kl, self.posb, self.popb = [], [], [], []\n",
    "        self.neg, self.kl, self.ubias = [], [], []\n",
    "\n",
    "        self.target = target\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def validate(self, cmd=\"valid\", k=100):\n",
    "        assert cmd in [\"valid\", \"test\"]\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        self.model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        n10_list, n100_list, r10_list, r100_list = [], [], [], []\n",
    "        embs_list = []\n",
    "        att_round, rel_round, cnt_round, pcount_round, udx_list = [], [], [], [], []\n",
    "        result = []\n",
    "        eval_loss = 0.0\n",
    "        eval_neg = 0.0\n",
    "        eval_kl = 0.0\n",
    "        eval_ubias = 0.0\n",
    "\n",
    "        loader_ = self.valid_loader if cmd == \"valid\" else self.test_loader\n",
    "\n",
    "        step_counter = 0\n",
    "        for batch_idx, (data_tr, data_te, prof, uindex, sens) in tqdm.tqdm(\n",
    "            enumerate(loader_),\n",
    "            total=len(loader_),\n",
    "            desc=\"{} check epoch={}, len={}\".format(\n",
    "                \"Valid\" if cmd == \"valid\" else \"Test\", self.epoch, len(loader_)\n",
    "            ),\n",
    "            ncols=80,\n",
    "            leave=False,\n",
    "        ):\n",
    "            step_counter = step_counter + 1\n",
    "\n",
    "            if self.cuda:\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "                sens=sens.cuda()\n",
    "            data_tr = Variable(data_tr)\n",
    "            prof = Variable(prof)\n",
    "            data_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, KL, mu_q, std_q, epsilon, sampled_z = self.model.forward(\n",
    "                    data_tr, prof\n",
    "                )\n",
    "\n",
    "                log_softmax_var = f.log_softmax(logits, dim=1)\n",
    "                neg_ll = -torch.mean(torch.sum(log_softmax_var * data_tr, dim=1))\n",
    "                eval_neg += neg_ll.item()\n",
    "                eval_kl += KL.item()\n",
    "\n",
    "                user_bias = utils.calc_user_bias(\n",
    "                    torch.sum(log_softmax_var * data_tr, dim=1), sens\n",
    "                )\n",
    "                eval_ubias += user_bias.item()\n",
    "\n",
    "                \n",
    "                ## SENSITIVE VAE ACCURACY \n",
    "                y_hat, mean, log_var = self.model.sensitive_atr_vae(sampled_z)\n",
    "                if self.cuda:\n",
    "                    class_loss = loss_function(torch.flatten(Variable(sens.type(torch.FloatTensor))).cuda(), torch.flatten(y_hat), mean, log_var)\n",
    "                else:\n",
    "                    class_loss = loss_function(torch.flatten(Variable(sens.type(torch.FloatTensor))), torch.flatten(y_hat), mean, log_var)\n",
    "\n",
    "\n",
    "                eval_loss += class_loss.item()\n",
    "\n",
    "                pred_val = logits.cpu().detach().numpy()\n",
    "                pred_val[data_tr.cpu().detach().numpy().nonzero()] = -np.inf\n",
    "\n",
    "                data_te_csr = sparse.csr_matrix(data_te.numpy())\n",
    "                n10_list.append(\n",
    "                    utils.NDCG_binary_at_k_batch(pred_val, data_te_csr, k=10)\n",
    "                )\n",
    "                n100_list.append(\n",
    "                    utils.NDCG_binary_at_k_batch(pred_val, data_te_csr, k=100)\n",
    "                )\n",
    "                r10_list.append(utils.Recall_at_k_batch(pred_val, data_te_csr, k=10))\n",
    "                r100_list.append(utils.Recall_at_k_batch(pred_val, data_te_csr, k=100))\n",
    "\n",
    "                if cmd == \"test\":\n",
    "                    for user in np.arange(data_te.numpy().shape[0]):\n",
    "                        dict_out = {}\n",
    "                        preds = pred_val[user, :]\n",
    "\n",
    "                        dict_out[\"num_missing_terms\"] = len(\n",
    "                            np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                        )\n",
    "                        dict_out[\"missing_terms\"] = \" \".join(\n",
    "                            [\n",
    "                                str(x)\n",
    "                                for x in list(\n",
    "                                    np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                        dict_out[\"num_terms\"] = len(\n",
    "                            np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                        ) + len(\n",
    "                            np.array(data_tr.cpu().detach().numpy()[user, :]).nonzero()[\n",
    "                                0\n",
    "                            ]\n",
    "                        )\n",
    "                        dict_out[\"recommended_terms\"] = \" \".join(\n",
    "                            [str(x) for x in list(np.argsort(-preds)[:k])]\n",
    "                        )\n",
    "                        dict_out[\"new_userId\"] = int(\n",
    "                            uindex[user].cpu().detach().numpy()\n",
    "                        )\n",
    "                        dict_out[\"scores\"] = \" \".join(\n",
    "                            [\n",
    "                                str(x)\n",
    "                                for x in list(np.sort(self.softmax(preds))[::-1][:k])\n",
    "                            ]\n",
    "                        )\n",
    "                        result.append(dict_out)\n",
    "\n",
    "        avg_loss = eval_loss / len(loader_)\n",
    "        avg_neg = eval_neg / len(loader_)\n",
    "        avg_kl = eval_kl / len(loader_)\n",
    "        avg_ubias = eval_ubias / len(loader_)\n",
    "\n",
    "        metrics = []\n",
    "        if cmd == \"valid\":\n",
    "            n10_list = np.concatenate(n10_list, axis=0)\n",
    "            n100_list = np.concatenate(n100_list, axis=0)\n",
    "            r10_list = np.concatenate(r10_list, axis=0)\n",
    "            r100_list = np.concatenate(r100_list, axis=0)\n",
    "\n",
    "            self.ndcg.append(np.mean(n100_list))\n",
    "            self.recall.append(np.mean(r100_list))\n",
    "            self.loss.append(avg_loss)\n",
    "            self.neg.append(avg_neg)\n",
    "            self.kl.append(avg_kl)\n",
    "            self.ubias.append(avg_ubias)\n",
    "\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_ndcg_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.ndcg,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_recall_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.recall,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_loss_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.loss,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_neg_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.neg,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_kl_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.kl,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_ubias_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.ubias,\n",
    "            )\n",
    "\n",
    "            # SAVE MODEL\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": self.epoch,\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optim.state_dict(),\n",
    "                },\n",
    "                self.checkpoint_dir\n",
    "                + self.dataset_name\n",
    "                + \"_vae_{}_{}_{}_{}.pth\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "            )\n",
    "            # with open(self.checkpoint_dir+self.dataset_name+'_vae_'+str(self.bias)+'_'+str(self.alpha)+'.pt', 'wb') as model_file: torch.save(self.model, model_file)\n",
    "            # torch.save({'state_dict': self.model.state_dict()}, self.checkpoint_dir+'vae')\n",
    "\n",
    "            metrics.append(\n",
    "                \"NDCG@10,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(n10_list), np.std(n10_list) / np.sqrt(len(n10_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"NDCG@100,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"Recall@10,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(r10_list), np.std(r10_list) / np.sqrt(len(r10_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"Recall@100,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(r100_list), np.std(r100_list) / np.sqrt(len(r100_list))\n",
    "                )\n",
    "            )\n",
    "            print(\"\\n\" + \",\\n\".join(metrics))\n",
    "\n",
    "        else:\n",
    "            final_results = pd.DataFrame(result)\n",
    "            final_results = final_results.merge(\n",
    "                self.user_mapper[[\"new_userId\", \"gender\", \"country\", \"age\"]],\n",
    "                on=\"new_userId\",\n",
    "                how=\"inner\",\n",
    "            )\n",
    "            final_results.to_csv(\n",
    "                \"results/{}_final_results_{}_{}_{}_{}.csv\".format(\n",
    "                    self.dataset_name, self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        cmd = \"train\"\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for batch_idx, (data_tr, data_te, prof, uidx, sens) in tqdm.tqdm(\n",
    "            enumerate(self.train_loader),\n",
    "            total=len(self.train_loader),\n",
    "            desc=\"Train check epoch={}, len={}\".format(\n",
    "                self.epoch, len(self.train_loader)\n",
    "            ),\n",
    "            ncols=80,\n",
    "            leave=False,\n",
    "        ):\n",
    "            self.step += 1\n",
    "\n",
    "            if self.cuda:\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "                # added by me\n",
    "                sens = sens.cuda()\n",
    "            data_tr = Variable(data_tr)\n",
    "            prof = Variable(prof)\n",
    "            data_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            logits, KL, mu_q, std_q, epsilon, sampled_z = self.model.forward(\n",
    "                data_tr, prof\n",
    "            )\n",
    "\n",
    "            log_softmax_var = f.log_softmax(logits, dim=1)\n",
    "            neg_ll = -torch.mean(torch.sum(log_softmax_var * data_tr, dim=1))\n",
    "\n",
    "            l2_reg = self.model.get_l2_reg()\n",
    "\n",
    "            if self.total_anneal_steps > 0:\n",
    "                self.anneal = min(self.beta, 1.0 * self.step / self.total_anneal_steps)\n",
    "            else:\n",
    "                self.anneal = self.beta\n",
    "\n",
    "            ## SENSITIVE VAE ACCURACY \n",
    "            y_hat, mean, log_var = self.model.sensitive_atr_vae(sampled_z)\n",
    "            if self.cuda:\n",
    "                class_loss = loss_function(torch.flatten(Variable(sens.type(torch.FloatTensor))).cuda(), torch.flatten(y_hat), mean, log_var)\n",
    "            else:\n",
    "                class_loss = loss_function(torch.flatten(Variable(sens.type(torch.FloatTensor))), torch.flatten(y_hat), mean, log_var)\n",
    "\n",
    "            # USER BIAS\n",
    "            user_bias = utils.calc_user_bias(\n",
    "                torch.sum(log_softmax_var * data_tr, dim=1), sens\n",
    "            )\n",
    "\n",
    "            loss = (\n",
    "                neg_ll\n",
    "                + self.anneal * KL\n",
    "                + l2_reg\n",
    "                - self.gamma * class_loss\n",
    "                + self.tau * user_bias\n",
    "            )\n",
    "\n",
    "            # backprop\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def train(self):\n",
    "        max_epoch = 100\n",
    "        for epoch in tqdm.trange(0, max_epoch, desc=\"Train\", ncols=80):\n",
    "            self.epoch = epoch\n",
    "            self.train_epoch()\n",
    "            self.lr_scheduler.step()\n",
    "            self.validate(cmd=\"valid\")\n",
    "            # self.validate(cmd='test')\n",
    "\n",
    "    def test(self):\n",
    "        self.validate(cmd=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.backends.cudnn.version: 8500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"torch.backends.cudnn.version: {}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "if not os.path.isdir(\"./checkpoint\"):\n",
    "    os.mkdir(\"./checkpoint\")\n",
    "if not os.path.isdir(\"./results\"):\n",
    "    os.mkdir(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=dict(\n",
    "        max_iteration=1000000,\n",
    "        lr=1e-4,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0,\n",
    "        gamma=0.1,  # \"lr_policy: step\"\n",
    "        step_size=200000,  # \"lr_policy: step\" e-6\n",
    "        interval_validate=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiVAE(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear_1): Linear(in_features=45759, out_features=2000, bias=True)\n",
      "    (linear_2): Linear(in_features=2000, out_features=400, bias=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (linear_1): Linear(in_features=200, out_features=2000, bias=True)\n",
      "    (linear_2): Linear(in_features=2000, out_features=45759, bias=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (sensitive_atr_vae): SENSITIVE_ATTR_VAE(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "      (2): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (mean_layer): Linear(in_features=50, out_features=2, bias=True)\n",
      "    (logvar_layer): Linear(in_features=50, out_features=2, bias=True)\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "      (2): Linear(in_features=50, out_features=100, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.2)\n",
      "      (4): Linear(in_features=100, out_features=1, bias=True)\n",
      "      (5): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultiVAE(\n",
    "    dropout_p=0.5,\n",
    "    weight_decay=0.0,\n",
    "    cuda2=cuda,\n",
    "    q_dims=[item_mapper.shape[0], 2000, 200],\n",
    "    p_dims=[200, 2000, item_mapper.shape[0]],\n",
    "    n_conditioned=0,\n",
    "    n_sensitive_attributes=1, # only country for now\n",
    ")\n",
    "# 3. optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=False)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=True)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ],\n",
    "    lr=cfg[\"lr\"],\n",
    ")\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_policy: step\n",
    "last_epoch = -1\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[50, 75], gamma=cfg[\"gamma\"], last_epoch=last_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|                                            | 0/100 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 350.00 MiB (GPU 0; 6.00 GiB total capacity; 4.07 GiB already allocated; 263.56 MiB free; 4.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\Data\\University\\Thesis Implementation\\implementation_temp.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     cmd\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cuda\u001b[39m=\u001b[39mcuda,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     target\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcountry_encoded\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[1;32md:\\Data\\University\\Thesis Implementation\\implementation_temp.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=399'>400</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtrange(\u001b[39m0\u001b[39m, max_epoch, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m\"\u001b[39m, ncols\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=400'>401</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=401'>402</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=402'>403</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=403'>404</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate(cmd\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Data\\University\\Thesis Implementation\\implementation_temp.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=388'>389</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=389'>390</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Data/University/Thesis%20Implementation/implementation_temp.ipynb#X23sZmlsZQ%3D%3D?line=390'>391</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[1;32md:\\Data\\Softwares\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Data\\Softwares\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data\\Softwares\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32md:\\Data\\Softwares\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Data\\Softwares\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32md:\\Data\\Softwares\\anaconda3\\envs\\thesis\\Lib\\site-packages\\torch\\optim\\adam.py:507\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    505\u001b[0m     exp_avg_sq_sqrt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m    506\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m--> 507\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_add(exp_avg_sq_sqrt, eps)\n\u001b[0;32m    509\u001b[0m torch\u001b[39m.\u001b[39m_foreach_addcdiv_(params_, device_exp_avgs, denom, step_size)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 350.00 MiB (GPU 0; 6.00 GiB total capacity; 4.07 GiB already allocated; 263.56 MiB free; 4.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    cmd=\"train\",\n",
    "    cuda=cuda,\n",
    "    model=model,\n",
    "    optim=optim,\n",
    "    gamma=0.5,\n",
    "    tau=0.5,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    # test_loader=test_loader,\n",
    "    start_step=0,\n",
    "    total_steps=int(3e5),\n",
    "    interval_validate=None,\n",
    "    checkpoint_dir=\"./checkpoint/\",\n",
    "    print_freq=1,\n",
    "    checkpoint_freq=1,\n",
    "    total_anneal_steps=2000,\n",
    "    beta=0.5,\n",
    "    item_mapper=item_mapper,\n",
    "    user_mapper=user_mapper,\n",
    "    dataset_name=\"lfm2b\",\n",
    "    # alpha=0.5,\n",
    "    base_dir=\"./Data/\",\n",
    "    target=[\"country_encoded\"],\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
