{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "import tqdm\n",
    "import time\n",
    "import datetime\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFM1bDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        item_mapper,\n",
    "        user_mapper,\n",
    "        target=[\"country\"],\n",
    "        fold_in=True,\n",
    "        split=\"train\",\n",
    "        conditioned_on=None,\n",
    "        upper=-1,\n",
    "    ):\n",
    "        super(LFM1bDataset, self).__init__()\n",
    "        assert os.path.exists(root), \"root: {} not found.\".format(root)\n",
    "        self.root = root\n",
    "\n",
    "        assert split in [\"test\", \"inference\", \"train\", \"valid\"]\n",
    "        self.split = split\n",
    "        \n",
    "\n",
    "        out_data_dir = root\n",
    "        self.target = target\n",
    "        self.user_mapper = user_mapper\n",
    "        self.class_list = sorted(pd.unique(self.user_mapper[self.target].values.ravel('K')).tolist())\n",
    "        if self.split == \"train\":\n",
    "            self.train_data = pd.read_csv(\n",
    "                root + \"user_interactions_train.csv\",\n",
    "                dtype=np.float64,\n",
    "                na_filter=False,\n",
    "                low_memory=False,\n",
    "            )\n",
    "        elif self.split == \"valid\":\n",
    "            self.vad_data_tr = pd.read_csv(\n",
    "                root + \"user_interactions_validation_tr.csv\",\n",
    "                dtype=np.float64,\n",
    "                na_filter=False,\n",
    "                low_memory=False,\n",
    "            )\n",
    "            self.vad_data_te = pd.read_csv(\n",
    "                root + \"user_interactions_validation_te.csv\",\n",
    "                dtype=np.float64,\n",
    "                na_filter=False,\n",
    "                low_memory=False,\n",
    "            )\n",
    "        elif self.split == \"test\":\n",
    "            self.test_data_tr = pd.read_csv(\n",
    "                root + \"user_interactions_test_tr.csv\",\n",
    "                dtype=np.float64,\n",
    "                na_filter=False,\n",
    "                low_memory=False,\n",
    "            )\n",
    "            self.test_data_te = pd.read_csv(\n",
    "                root + \"user_interactions_test_te.csv\",\n",
    "                dtype=np.float64,\n",
    "                na_filter=False,\n",
    "                low_memory=False,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            self.n_users = self.train_data.shape[0]\n",
    "        elif self.split == \"valid\":\n",
    "            self.n_users = self.vad_data_tr.shape[0]\n",
    "        elif self.split == \"test\":\n",
    "            self.n_users = self.test_data_tr.shape[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def encode_label(self, label, class_list):\n",
    "        target = np.zeros(len(class_list))\n",
    "        for l in label:\n",
    "            idx = class_list.index(l)\n",
    "            target[idx] = 1\n",
    "        return target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prof = np.zeros(1)\n",
    "        if self.split == \"train\":\n",
    "            data_tr, data_te = self.train_data.iloc[index].drop(\"user_id\").to_numpy(\n",
    "                dtype=\"float32\"\n",
    "            ), np.zeros(1)\n",
    "            idx_user = self.train_data.at[index, \"user_id\"]\n",
    "        elif self.split == \"valid\":\n",
    "            # un comment line when vad_data_te is available\n",
    "            data_tr, data_te = self.vad_data_tr.iloc[index].drop(\"user_id\").to_numpy(\n",
    "                dtype=\"float32\"\n",
    "            ), self.vad_data_te.iloc[index].drop(\"user_id\").to_numpy(dtype=\"float32\")\n",
    "            # data_tr, data_te = self.vad_data_tr.iloc[index].drop('user_id').to_numpy(dtype='float32'), np.zeros(1)\n",
    "            idx_user = self.vad_data_tr.at[index, \"user_id\"]\n",
    "        elif self.split == \"test\":\n",
    "            data_tr, data_te = self.test_data_tr.iloc[index].drop(\"user_id\").to_numpy(\n",
    "                dtype=\"float32\"\n",
    "            ), self.test_data_te.iloc[index].drop(\"user_id\").to_numpy(dtype=\"float32\")\n",
    "            idx_user = self.test_data_tr.at[index, \"user_id\"]\n",
    "\n",
    "        sensitive = self.user_mapper.loc[self.user_mapper.user_id == idx_user][\n",
    "            self.target\n",
    "        ].values[0]\n",
    "        class_target = self.encode_label(\n",
    "            label=sensitive,\n",
    "            class_list=self.class_list,\n",
    "        )\n",
    "        print(\"\\nclass_list {}\".format(self.class_list))\n",
    "        print(\"\\nuser sensitive attributes {}\".format(sensitive))\n",
    "        print(\"\\nencoded class targets {}\".format(class_target))\n",
    "        return data_tr, data_te, prof, idx_user, class_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mapper = pd.read_csv('./Data/items.csv')\n",
    "user_mapper = pd.read_csv('./Data/users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SENSITIVE_ATTR_VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim=200, n_sensitive_attributes=1, hidden_dim=100, latent_dim=50\n",
    "    ):\n",
    "        super(SENSITIVE_ATTR_VAE, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # latent mean and variance\n",
    "        self.mean_layer = nn.Linear(latent_dim, 2)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 2)\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_sensitive_attributes),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(self.device)\n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "\n",
    "\n",
    "def loss_function(x_hat, mean, log_var, sensitive_attr):\n",
    "\n",
    "    # Separate country and gender labels\n",
    "    country_labels = sensitive_attr[:, 0]\n",
    "\n",
    "    # Mean Squared Error (MSE) loss for country\n",
    "    mse_country_loss = nn.functional.mse_loss(x_hat[:, 0], country_labels)\n",
    "\n",
    "    # # Mean Squared Error (MSE) loss for gender\n",
    "    # mse_gender_loss = nn.functional.mse_loss(output[:, 1], gender_labels)\n",
    "\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    # Adjust the weighting factor for the classification losses based on your needs\n",
    "    total_loss = (1 * mse_country_loss) + kl_loss  # + (1 * gender_loss)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summed_unique_values_count(df, columns):\n",
    "    # Get the count of unique values for the specified columns\n",
    "    unique_values_count = df[columns].nunique()\n",
    "\n",
    "    # Sum the unique values counts across columns\n",
    "    total_count = unique_values_count.sum()\n",
    "\n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a deeper neural network architecture\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_size=200, hidden_size1=128, hidden_size2=64, num_classes=None):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, options, dropout_p=0.5, q_dims=[20108, 600, 200]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.options = options\n",
    "        self.q_dims = q_dims\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p, inplace=False)\n",
    "        self.linear_1 = nn.Linear(self.q_dims[0], self.q_dims[1], bias=True)\n",
    "        self.linear_2 = nn.Linear(self.q_dims[1], self.q_dims[2] * 2, bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        for module_name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dropout(x) \n",
    "        x = self.linear_1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear_2(x)\n",
    "        mu_q, logvar_q = torch.chunk(x, chunks=2, dim=1)\n",
    "        return mu_q, logvar_q\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, options, p_dims=[200, 600, 20108]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.options = options\n",
    "        self.p_dims = p_dims\n",
    "\n",
    "        self.linear_1 = nn.Linear(self.p_dims[0], self.p_dims[1], bias=True)\n",
    "        self.linear_2 = nn.Linear(self.p_dims[1], self.p_dims[2], bias=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        for module_name, m in self.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class MultiVAE(nn.Module):\n",
    "    def __init__(self, cuda2=True, weight_decay=0.0, dropout_p=0.5, q_dims=[20108, 600, 200], p_dims=[200, 600, 20108], n_conditioned=0, sensitive_attributes=[\"country\"]):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.cuda2 = cuda2\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_conditioned = n_conditioned\n",
    "        self.q_dims = q_dims\n",
    "        self.p_dims = p_dims\n",
    "        self.q_dims[0] += self.n_conditioned\n",
    "        self.p_dims[0] += self.n_conditioned\n",
    "\n",
    "        self.encoder = Encoder(None, dropout_p=dropout_p, q_dims=self.q_dims)\n",
    "        self.decoder = Decoder(None, p_dims=self.p_dims)\n",
    "\n",
    "        # self.sensitive_atr_vae = SENSITIVE_ATTR_VAE(n_sensitive_attributes=n_sensitive_attributes)\n",
    "        self.multi_label_classifier=MultiLabelClassifier(input_size=200,num_classes=get_summed_unique_values_count(user_mapper,columns=sensitive_attributes))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x = f.normalize(x, p=2, dim=1)\n",
    "        if self.n_conditioned > 0:\n",
    "            x = torch.cat((x, c), dim=1)\n",
    "        mu_q, logvar_q = self.encoder.forward(x)\n",
    "        std_q = torch.exp(0.5 * logvar_q)\n",
    "        KL = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q ** 2 - 1), dim=1))\n",
    "\n",
    "        if True:\n",
    "            if self.training:\n",
    "                epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "                sampled_z = mu_q + epsilon * std_q\n",
    "            else:\n",
    "                epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "                sampled_z = mu_q\n",
    "        else:\n",
    "            epsilon = torch.randn_like(std_q, requires_grad=False)\n",
    "            sampled_z = mu_q + epsilon * std_q\n",
    "\n",
    "        if self.n_conditioned > 0:\n",
    "            sampled_z = torch.cat((sampled_z, c), dim=1)\n",
    "        logits = self.decoder.forward(sampled_z)\n",
    "\n",
    "        return logits, KL, mu_q, std_q, epsilon, sampled_z\n",
    "\n",
    "    def get_l2_reg(self):\n",
    "        l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "        if self.weight_decay > 0:\n",
    "            for k, m in self.state_dict().items():\n",
    "                if k.endswith('.weight'):\n",
    "                    l2_reg = l2_reg + torch.norm(m, p=2) ** 2\n",
    "        if self.cuda2:\n",
    "            l2_reg = l2_reg.cuda()\n",
    "        return self.weight_decay * l2_reg[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = LFM1bDataset\n",
    "dt = DS(\n",
    "    './Data/',\n",
    "    item_mapper,\n",
    "    user_mapper,\n",
    "    target=['country'],\n",
    "    split=\"train\"\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dt, batch_size=500, shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DS(\n",
    "'./Data/',\n",
    "item_mapper,\n",
    "user_mapper,\n",
    "target=['country'],\n",
    "split=\"valid\",\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "dt, batch_size=200, shuffle=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cmd,\n",
    "        cuda,\n",
    "        model,\n",
    "        optim=None,\n",
    "        train_loader=None,\n",
    "        valid_loader=None,\n",
    "        test_loader=None,\n",
    "        log_file=None,\n",
    "        interval_validate=1,\n",
    "        lr_scheduler=None,\n",
    "        dataset_name=None,\n",
    "        gamma=0.0,\n",
    "        tau=0.0,\n",
    "        start_step=0,\n",
    "        total_steps=1e5,\n",
    "        start_epoch=0,\n",
    "        bias=False,\n",
    "        target=None,\n",
    "        total_anneal_steps=200000,\n",
    "        beta=0.1,\n",
    "        do_normalize=True,\n",
    "        item_mapper=None,\n",
    "        user_mapper=None,\n",
    "        checkpoint_dir=None,\n",
    "        result_dir=None,\n",
    "        print_freq=1,\n",
    "        result_save_freq=1,\n",
    "        checkpoint_freq=1,\n",
    "        base_dir=None,\n",
    "    ):\n",
    "        self.cmd = cmd\n",
    "        self.cuda = cuda\n",
    "        self.model = model\n",
    "        self.item_mapper = item_mapper\n",
    "        self.user_mapper = user_mapper\n",
    "        self.dataset_name = dataset_name\n",
    "        self.bias = bias\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        self.optim = optim\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.timestamp_start = datetime.datetime.now()\n",
    "\n",
    "        if self.cmd == \"train\":\n",
    "            self.interval_validate = interval_validate\n",
    "\n",
    "        self.start_step = start_step\n",
    "        self.step = start_step\n",
    "        self.total_steps = total_steps\n",
    "        self.epoch = start_epoch\n",
    "\n",
    "        self.do_normalize = do_normalize\n",
    "        self.print_freq = print_freq\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.total_anneal_steps = total_anneal_steps\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.ndcg, self.recall, self.ash, self.amt, self.alt, self.ent, self.demo = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        self.loss, self.kl, self.posb, self.popb = [], [], [], []\n",
    "        self.neg, self.kl, self.ubias = [], [], []\n",
    "\n",
    "        self.target = target\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.classifier_criterion=torch.nn.BCELoss()\n",
    "\n",
    "    def validate(self, cmd=\"valid\", k=100):\n",
    "        assert cmd in [\"valid\", \"test\"]\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        self.model.eval()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        n10_list, n100_list, r10_list, r100_list = [], [], [], []\n",
    "        embs_list = []\n",
    "        att_round, rel_round, cnt_round, pcount_round, udx_list = [], [], [], [], []\n",
    "        result = []\n",
    "        eval_loss = 0.0\n",
    "        eval_neg = 0.0\n",
    "        eval_kl = 0.0\n",
    "        eval_ubias = 0.0\n",
    "\n",
    "        loader_ = self.valid_loader if cmd == \"valid\" else self.test_loader\n",
    "\n",
    "        step_counter = 0\n",
    "        for batch_idx, (data_tr, data_te, prof, uindex,class_target) in tqdm.tqdm(\n",
    "            enumerate(loader_),\n",
    "            total=len(loader_),\n",
    "            desc=\"{} check epoch={}, len={}\".format(\n",
    "                \"Valid\" if cmd == \"valid\" else \"Test\", self.epoch, len(loader_)\n",
    "            ),\n",
    "            ncols=80,\n",
    "            leave=False,\n",
    "        ):\n",
    "            step_counter = step_counter + 1\n",
    "\n",
    "            if self.cuda:\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "                class_target=class_target.cuda()\n",
    "            data_tr = Variable(data_tr)\n",
    "            prof = Variable(prof)\n",
    "            data_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, KL, mu_q, std_q, epsilon, sampled_z = self.model.forward(\n",
    "                    data_tr, prof\n",
    "                )\n",
    "\n",
    "                log_softmax_var = f.log_softmax(logits, dim=1)\n",
    "                neg_ll = -torch.mean(torch.sum(log_softmax_var * data_tr, dim=1))\n",
    "                eval_neg += neg_ll.item()\n",
    "                eval_kl += KL.item()\n",
    "\n",
    "                user_bias = utils.calc_user_bias(\n",
    "                    torch.sum(log_softmax_var * data_tr, dim=1), class_target\n",
    "                )\n",
    "                eval_ubias += user_bias.item()\n",
    "\n",
    "                \n",
    "                ## SENSITIVE VAE ACCURACY \n",
    "                # y_hat, mean, log_var = self.model.sensitive_atr_vae(sampled_z)\n",
    "                y_hat=self.model.multi_label_classifier(sampled_z)\n",
    "                \n",
    "                if self.cuda:\n",
    "                    # class_loss = loss_function(sensitive_attr=Variable(sens.type(torch.FloatTensor)).cuda(), x_hat=y_hat, mean=mean, log_var=log_var)\n",
    "                    class_loss=self.classifier_criterion(y_hat,Variable(class_target.type(torch.FloatTensor)).cuda())\n",
    "                else:\n",
    "                    # class_loss = loss_function(sensitive_attr=Variable(sens.type(torch.FloatTensor)), x_hat=y_hat, mean=mean, log_var=log_var)\n",
    "                    class_loss=self.classifier_criterion(y_hat,Variable(class_target.type(torch.FloatTensor)))\n",
    "\n",
    "\n",
    "                eval_loss += class_loss.item()\n",
    "\n",
    "                pred_val = logits.cpu().detach().numpy()\n",
    "                pred_val[data_tr.cpu().detach().numpy().nonzero()] = -np.inf\n",
    "\n",
    "                data_te_csr = sparse.csr_matrix(data_te.numpy())\n",
    "                n10_list.append(\n",
    "                    utils.NDCG_binary_at_k_batch(pred_val, data_te_csr, k=10)\n",
    "                )\n",
    "                n100_list.append(\n",
    "                    utils.NDCG_binary_at_k_batch(pred_val, data_te_csr, k=100)\n",
    "                )\n",
    "                r10_list.append(utils.Recall_at_k_batch(pred_val, data_te_csr, k=10))\n",
    "                r100_list.append(utils.Recall_at_k_batch(pred_val, data_te_csr, k=100))\n",
    "\n",
    "                if cmd == \"test\":\n",
    "                    for user in np.arange(data_te.numpy().shape[0]):\n",
    "                        dict_out = {}\n",
    "                        preds = pred_val[user, :]\n",
    "\n",
    "                        dict_out[\"num_missing_terms\"] = len(\n",
    "                            np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                        )\n",
    "                        dict_out[\"missing_terms\"] = \" \".join(\n",
    "                            [\n",
    "                                str(x)\n",
    "                                for x in list(\n",
    "                                    np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                        dict_out[\"num_terms\"] = len(\n",
    "                            np.array(data_te.numpy()[user, :]).nonzero()[0]\n",
    "                        ) + len(\n",
    "                            np.array(data_tr.cpu().detach().numpy()[user, :]).nonzero()[\n",
    "                                0\n",
    "                            ]\n",
    "                        )\n",
    "                        dict_out[\"recommended_terms\"] = \" \".join(\n",
    "                            [str(x) for x in list(np.argsort(-preds)[:k])]\n",
    "                        )\n",
    "                        dict_out[\"user_id\"] = int(\n",
    "                            uindex[user].cpu().detach().numpy()\n",
    "                        )\n",
    "                        dict_out[\"scores\"] = \" \".join(\n",
    "                            [\n",
    "                                str(x)\n",
    "                                for x in list(np.sort(self.softmax(preds))[::-1][:k])\n",
    "                            ]\n",
    "                        )\n",
    "                        result.append(dict_out)\n",
    "\n",
    "        avg_loss = eval_loss / len(loader_)\n",
    "        avg_neg = eval_neg / len(loader_)\n",
    "        avg_kl = eval_kl / len(loader_)\n",
    "        avg_ubias = eval_ubias / len(loader_)\n",
    "\n",
    "        metrics = []\n",
    "        if cmd == \"valid\":\n",
    "            n10_list = np.concatenate(n10_list, axis=0)\n",
    "            n100_list = np.concatenate(n100_list, axis=0)\n",
    "            r10_list = np.concatenate(r10_list, axis=0)\n",
    "            r100_list = np.concatenate(r100_list, axis=0)\n",
    "\n",
    "            self.ndcg.append(np.mean(n100_list))\n",
    "            self.recall.append(np.mean(r100_list))\n",
    "            self.loss.append(avg_loss)\n",
    "            self.neg.append(avg_neg)\n",
    "            self.kl.append(avg_kl)\n",
    "            self.ubias.append(avg_ubias)\n",
    "\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_ndcg_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.ndcg,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_recall_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.recall,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_loss_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.loss,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_neg_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.neg,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_kl_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.kl,\n",
    "            )\n",
    "            np.save(\n",
    "                \"results/\"\n",
    "                + self.dataset_name\n",
    "                + \"_ubias_{}_{}_{}_{}.npy\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                self.ubias,\n",
    "            )\n",
    "\n",
    "            # SAVE MODEL\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": self.epoch,\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optim.state_dict(),\n",
    "                },\n",
    "                self.checkpoint_dir\n",
    "                + self.dataset_name\n",
    "                + \"_vae_{}_{}_{}_{}.pth\".format(\n",
    "                    self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "            )\n",
    "            # with open(self.checkpoint_dir+self.dataset_name+'_vae_'+str(self.bias)+'_'+str(self.alpha)+'.pt', 'wb') as model_file: torch.save(self.model, model_file)\n",
    "            # torch.save({'state_dict': self.model.state_dict()}, self.checkpoint_dir+'vae')\n",
    "\n",
    "            metrics.append(\n",
    "                \"NDCG@10,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(n10_list), np.std(n10_list) / np.sqrt(len(n10_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"NDCG@100,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"Recall@10,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(r10_list), np.std(r10_list) / np.sqrt(len(r10_list))\n",
    "                )\n",
    "            )\n",
    "            metrics.append(\n",
    "                \"Recall@100,{:.5f},{:.5f}\".format(\n",
    "                    np.mean(r100_list), np.std(r100_list) / np.sqrt(len(r100_list))\n",
    "                )\n",
    "            )\n",
    "            print(\"\\n\" + \",\\n\".join(metrics))\n",
    "\n",
    "        else:\n",
    "            final_results = pd.DataFrame(result)\n",
    "            final_results = final_results.merge(\n",
    "                self.user_mapper[[\"user_id\", \"sex\", \"country\", \"age\"]],\n",
    "                on=\"user_id\",\n",
    "                how=\"inner\",\n",
    "            )\n",
    "            final_results.to_csv(\n",
    "                \"results/{}_final_results_{}_{}_{}_{}.csv\".format(\n",
    "                    self.dataset_name, self.target, self.beta, self.gamma, self.tau\n",
    "                ),\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        cmd = \"train\"\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "        for batch_idx, (data_tr, data_te, prof, uidx, class_target) in tqdm.tqdm(\n",
    "            enumerate(self.train_loader),\n",
    "            total=len(self.train_loader),\n",
    "            desc=\"Train check epoch={}, len={}\".format(\n",
    "                self.epoch, len(self.train_loader)\n",
    "            ),\n",
    "            ncols=80,\n",
    "            leave=False,\n",
    "        ):\n",
    "            self.step += 1\n",
    "\n",
    "            if self.cuda:\n",
    "                data_tr = data_tr.cuda()\n",
    "                prof = prof.cuda()\n",
    "                # added by me\n",
    "                class_target = class_target.cuda()\n",
    "            data_tr = Variable(data_tr)\n",
    "            prof = Variable(prof)\n",
    "            data_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            logits, KL, mu_q, std_q, epsilon, sampled_z = self.model.forward(\n",
    "                data_tr, prof\n",
    "            )\n",
    "            \n",
    "            log_softmax_var = f.log_softmax(logits, dim=1)\n",
    "            neg_ll = -torch.mean(torch.sum(log_softmax_var * data_tr, dim=1))\n",
    "\n",
    "            l2_reg = self.model.get_l2_reg()\n",
    "\n",
    "            if self.total_anneal_steps > 0:\n",
    "                self.anneal = min(self.beta, 1.0 * self.step / self.total_anneal_steps)\n",
    "            else:\n",
    "                self.anneal = self.beta\n",
    "\n",
    "            ## SENSITIVE VAE ACCURACY \n",
    "            # y_hat, mean, log_var = self.model.sensitive_atr_vae(sampled_z)\n",
    "            y_hat=self.model.multi_label_classifier(sampled_z)\n",
    "            if self.cuda:\n",
    "                # class_loss = loss_function(sensitive_attr=Variable(sens.type(torch.FloatTensor)).cuda(), x_hat=y_hat, mean=mean, log_var=log_var)\n",
    "                class_loss=self.classifier_criterion(y_hat,Variable(class_target.type(torch.FloatTensor)).cuda())\n",
    "            else:\n",
    "                # class_loss = loss_function(sensitive_attr=Variable(sens.type(torch.FloatTensor)), x_hat=y_hat, mean=mean, log_var=log_var)\n",
    "                class_loss=self.classifier_criterion(y_hat,Variable(class_target.type(torch.FloatTensor)))\n",
    "           \n",
    "            # USER BIAS\n",
    "            user_bias = utils.calc_user_bias(\n",
    "                torch.sum(log_softmax_var * data_tr, dim=1), class_target\n",
    "            )\n",
    "\n",
    "            loss = (\n",
    "                neg_ll\n",
    "                + self.anneal * KL\n",
    "                + l2_reg\n",
    "                - self.gamma * class_loss\n",
    "                + self.tau * user_bias\n",
    "            )\n",
    "            print(\"Total loss: {}\\n\".format(loss/len(data_tr)))\n",
    "            # backprop\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def train(self):\n",
    "        max_epoch = 100\n",
    "        for epoch in tqdm.trange(0, max_epoch, desc=\"Train\", ncols=80):\n",
    "            self.epoch = epoch\n",
    "            self.train_epoch()\n",
    "            self.lr_scheduler.step()\n",
    "            self.validate(cmd=\"valid\")\n",
    "            # self.validate(cmd='test')\n",
    "\n",
    "    def test(self):\n",
    "        self.validate(cmd=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(\"torch.backends.cudnn.version: {}\".format(torch.backends.cudnn.version()))\n",
    "\n",
    "if not os.path.isdir(\"./checkpoint\"):\n",
    "    os.mkdir(\"./checkpoint\")\n",
    "if not os.path.isdir(\"./results\"):\n",
    "    os.mkdir(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=dict(\n",
    "        max_iteration=1000000,\n",
    "        lr=1e-4,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0,\n",
    "        gamma=0.1,  # \"lr_policy: step\"\n",
    "        step_size=200000,  # \"lr_policy: step\" e-6\n",
    "        interval_validate=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiVAE(\n",
    "    dropout_p=0.5,\n",
    "    weight_decay=0.0,\n",
    "    cuda2=cuda,\n",
    "    q_dims=[item_mapper.shape[0], 2000, 200],\n",
    "    p_dims=[200, 2000, item_mapper.shape[0]],\n",
    "    n_conditioned=0,\n",
    "    sensitive_attributes=[\"country\"], # only country for now\n",
    ")\n",
    "# 3. optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=False)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=True)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ],\n",
    "    lr=cfg[\"lr\"],\n",
    ")\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_policy: step\n",
    "last_epoch = -1\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[50, 75], gamma=cfg[\"gamma\"], last_epoch=last_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    cmd=\"train\",\n",
    "    cuda=cuda,\n",
    "    model=model,\n",
    "    optim=optim,\n",
    "    gamma=0.5,\n",
    "    tau=0.5,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    # test_loader=test_loader,\n",
    "    start_step=0,\n",
    "    total_steps=int(3e5),\n",
    "    interval_validate=None,\n",
    "    checkpoint_dir=\"./checkpoint/\",\n",
    "    print_freq=1,\n",
    "    checkpoint_freq=1,\n",
    "    total_anneal_steps=2000,\n",
    "    beta=0.5,\n",
    "    item_mapper=item_mapper,\n",
    "    user_mapper=user_mapper,\n",
    "    dataset_name=\"lfm2b\",\n",
    "    # alpha=0.5,\n",
    "    base_dir=\"./Data/\",\n",
    "    target=[\"country\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = LFM1bDataset\n",
    "df_test = DS(\n",
    "    \"./Data/\",\n",
    "    item_mapper,\n",
    "    user_mapper,\n",
    "    target=[\"country_encoded\"],\n",
    "    split=\"test\",\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    df_test,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiVAE(\n",
    "    dropout_p=0.5,\n",
    "    weight_decay=0.0,\n",
    "    cuda2=cuda,\n",
    "    q_dims=[item_mapper.shape[0], 2000, 200],\n",
    "    p_dims=[200, 2000, item_mapper.shape[0]],\n",
    "    n_conditioned=0,\n",
    "    n_sensitive_attributes=1,  # only country for now\n",
    ")\n",
    "# 3. optimizer\n",
    "optim = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=False)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": list(utils.get_parameters(model, bias=True)),\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ],\n",
    "    lr=cfg[\"lr\"],\n",
    ")\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "checkpoint = torch.load(\n",
    "    \"checkpoint/{}_vae_{}_{}_{}_{}.pth\".format(\n",
    "        \"lfm2b\", [\"country_encoded\"], 0.5, 0.5, 0.5\n",
    "    )\n",
    ")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optim.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "epoch = checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = -1\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optim, milestones=[50, 75], gamma=cfg[\"gamma\"], last_epoch=last_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    cmd=\"test\",\n",
    "    cuda=cuda,\n",
    "    model=model,\n",
    "    optim=optim,\n",
    "    gamma=0.5,\n",
    "    tau=0.5,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    train_loader=None,\n",
    "    valid_loader=None,\n",
    "    test_loader=test_loader,\n",
    "    start_step=0,\n",
    "    total_steps=int(3e5),\n",
    "    interval_validate=None,\n",
    "    checkpoint_dir=\"./checkpoint/\",\n",
    "    print_freq=1,\n",
    "    checkpoint_freq=1,\n",
    "    total_anneal_steps=2000,\n",
    "    beta=0.5,\n",
    "    item_mapper=item_mapper,\n",
    "    user_mapper=user_mapper,\n",
    "    dataset_name=\"lfm2b\",\n",
    "    # alpha=0.5,\n",
    "    base_dir=\"./Data/\",\n",
    "    target=[\"country_encoded\"],\n",
    ")\n",
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
